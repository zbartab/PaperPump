---
title: Publication patterns in Evolution and Ecology
author: ZoltÃ¡n Barta; Tibor Magura
date: !__DATA__
---

---
bibliography: /home/apa/notes/articles.bib
natbib: true
biblio-files: /home/apa/notes/articles.bib
biblio-style: /home/apa/lib/texinputs/AmNat
biblio-title: References
fontsize: 12pt
papersize: a4paper
include-before:
- \linenumbers
header-includes:
- \usepackage{lineno}
- \usepackage{double_spaced}
geometry:
- margin=1in
---

&define pub publication
&define pubnet publication network
&define colnet collaboration network
&define pubcart publication cartel
&define pubmat publication matrix
&define pubmats publication matrices
&define colmat collaboration matrix
&define colmats collaboration matrices
&define susgroup suspicious group

# TODOs

- calculate statistics for cut off values other than 0.4
	- plot the number of guilds, percentage of authors in guilds and percentage of edged affected as a function of the cut off value
- use the MTMT dataset to calculate cartels for different scientific fields
- implement calculation of strength
- description of the publication databases
	- fit power-law distributions

# Set up

Here we load some packages and custom codes.

```julia
## set the working directory
## load the libraries
using StatsBase, Statistics, PyPlot, Compose, Cairo
using LightGraphs, MetaGraphs, GraphPlot
## load local code
include("CollaborationNetworks.jl")
include("PlotCollaborationNetworks.jl")
include("simulations/RandomPublicationNetworks.jl")
#include("ProcessMTMTrecords.jl")
#include("ProcessDBLPrecords.jl")
```

# Random networks



# The datasets

&define mtmt __MTMT__
&define dblp __dblp__

TODO: clarify description of data sources

We use several publication database to investigate possible publication cartels. After downloading publication records we collected for each authors for whom we have data the papers the given author contributed to. From these data we construct a !pubmat, _M_, where rows represent papers while columns represent authors. The _m_~i,j~ element of _M_ gives if author _j_ contributed to paper _i_ (_m_~i,j~ = 1) or not (_m_~i,j~ = 0). Basically, _M_ represents a bipartite graph with authors and papers being the two types of nodes. From the !pubmat we constructed a weighted !colmat by projecting the bipartite matrix to the authors. The weight in the !colmat between author _i_ and _j_ represents the proportion of shared publications, $p_{i,j}$: 

$$
p_{i,j} = \frac{|A_i \cap A_j|}{|A_i \cup A_j|}
$$

Here, _A_~i~ is the set of papers to which author _i_ contributed. In other words, the weight between two authors is the number of shared papers divided by the total number of unique papers to which either of authors _i_ and _j_ contributed to. This weight is also called as Jaccard similarity. It varies between zero (i.e. no common publication between author _i_ and _j_) and one (i.e. all publication by the two authors are shared). Finally, we transformed the !colmats to weighted undirected !colnets.

The first database is the Hungarian bibliographic database, [!mtmt](https://www.mtmt.hu). Obtaining data from !mtmt is described in the [first attempt](publication_patterns.Rmd) of analysing !pubcarts. 
The details of processing !mtmt data is organised and documented in a [Makefile](MTMT/Makefile).

The other database is [dblp](https://dblp.org), which is a computer science bibliography database. The details of obtaining and processing !dblp data is documented in a [Makefile](dblp/Makefile).

The !dblp datasets contain many authors who have only one paper in the database. Having those authors can distort the collaboration relationship because all those authors who have only one paper in the dataset but their share that paper will have a very strong weight. To reduce this distortion we remove those authors from the !pubmat who have only three or less papers when we produced the !colmat. To be consistent we also filtered these authors out from the !mtmt database. All computation on !colmats are based on !colmats produced from these reduced !pubmats.

Here we load and process the available datasets and produce !pubmats, !colmats and !colnets. We store these matrices and graphs in dictionaries for latter processing.

```julia
pubmats = Dict()
colmats = Dict()
colnets = Dict()
pubmats[:MTMT] = read_scimat("MTMT/MTMTpubmat.txt")
colmats[:MTMT] = read_scimat("MTMT/MTMTcolmat.txt")
colnets[:MTMT] = collaborationgraph(colmats[:MTMT])
pubmats[:dblp] = read_scimat("dblp/dblppubmat.txt")
colmats[:dblp] = read_scimat("dblp/dblpcolmat.txt")
colnets[:dblp] = collaborationgraph(colmats[:dblp])
```

# Analyses of the publication and collaboration networks

## Description of the !pub dataset

The distribution of the number of papers contributed to by an author.

```julia
for k in keys(pubmats)
	no_papers = papernumbers(pubmats[k])
	onepaperers = sum(no_papers .== 1)
	println("Number of one-paper authors ($(k) database): ", onepaperers)
	println("Their proportion to all authors: ", onepaperers/length(no_papers))
	plotloglog(no_papers, k, "Number of papers per author")
	describe(no_papers)
end
```


The distribution of the number of authors per paper.

```julia
for k in keys(pubmats)
	no_coauthors = authornumbers(pubmats[k])
	plotloglog(no_coauthors, k, "Number of authors per paper")
	println(k, " database")
	describe(no_coauthors)
end
```

The distribution of the number of coauthors.

```julia
for k in keys(colnets)
	global no_coau = degree(colnets[k])
	hi_coau = histogram(no_coau)
	#plothistogram(hi_coau, xlab="Number of coauthors", ylab="Frequency")
	plotloglog(no_coau, k, "Number of coauthors")
	println("\n", k, " database")
	describe(no_coau)
end
```

The distribution of weights between coauthors.

```julia
for k in keys(colnets)
	println(k)
	W = Weights(colnets[k])
	hist(W, 0.0:0.010:1.0, label=k)
	xlabel("Edge weight")
	ylabel("Frequency")
	tight_layout()
	println("\n", k, " database")
	describe(W)
end
```

## Analysing !susgroups

```julia
for k in keys(colnets)
	d = describecartels(colnets[k])
	println("\n", k, " database")
	println(d)
	plothistogram(d["cartel_sizes"], plottitle="Frequency of cartel sizes",
		xlab="cartel size", ylab="relative frequency", relfreq=true)
end
```

```julia
function myECDF(x)
	xs = sort(x)
	y = (1:length(xs)) ./ length(xs)
	Dict(:xs => xs, :y => y)
end
rpubmat = Dict()
rcolmat = Dict()
rcolnet = Dict()
rpubmat[:dblp1] = read_scimat("dblp/dblppubmat-rewired-1.txt")
rpubmat[:MTMT1] = read_scimat("MTMT/MTMTpubmat-rewired-1.txt")
rcolmat[:dblp1] = read_scimat("dblp/dblpcolmat-rewired-1.txt")
rcolmat[:MTMT1] = read_scimat("MTMT/MTMTcolmat-rewired-1.txt")
rcolnet[:dblp1] = collaborationgraph(rcolmat[:dblp1])
rcolnet[:MTMT1] = collaborationgraph(rcolmat[:MTMT1])
Ws = Dict()
Ws[:dblp] = Weights(colnets[:dblp])
Ws[:MTMT] = Weights(colnets[:MTMT])
Ws[:dblp1] = Weights(rcolnet[:dblp1])
Ws[:MTMT1] = Weights(rcolnet[:MTMT1])
for k in keys(Ws)
	a = myECDF(Ws[k])
	#plot(log10.(a[:xs]), log10.(a[:y]), label=k)
	plot(a[:xs], a[:y], label=k)
end
legend()
Ws = Dict()
Ws[:dblp] = degree(colnets[:dblp])
Ws[:MTMT] = degree(colnets[:MTMT])
Ws[:dblp1] = degree(rcolnet[:dblp1])
Ws[:MTMT1] = degree(rcolnet[:MTMT1])
for k in sort(collect(keys(Ws)))
	plotloglog(Ws[k], k, "")
end
legend()
```


```julia
re_pubmat = Dict()
re_colmat = Dict()
for i in 1:20
	re_pubmat[i] = rewire(MTMTpubmat, 1_000_000)
	re_colmat[i] = collaborationmatrix(re_pubmat[i])
end
for i in 1:20
	println("MTMTpubmat_rewired-$(i).csv")
end
```



### Calculate publication measures

We calculate a couple of derived bibliographic measures for each author:

- number of papers
- total number of independent citations
- Hirsch index
- total number of coauthors
- number of D1 papers
- average number of citations per papers
- proportion of D1 papers
- number of papers weighted by
	- the equal contribution scheme
	- the sequence determined contribution scheme
	- the first, last author emphasis scheme
	- the bonus for first authorship scheme
	- the bonus for last authorship scheme
- number of independent citations weighted by the same schemes as used for the number of papers
- number of D1 papers weighted by the same schemes as used for the number of papers

```julia
sum.pubs <- NULL
for(s in papers.ls) {
	r <- calc.bib.measures(s)
	if(is.null(sum.pubs)) {
		sum.pubs <- data.frame(t(r))
	} else {
		sum.pubs <- rbind(sum.pubs, r)
	}
}
rownames(sum.pubs) <- names(papers.ls)
```



### Productivity of guilds

First we identify authors part of a guild in the productivity database.

```julia
full.au <- sum.pubs
guild.authors <- names(V(g.pubs.st.red))
non.guild.authors <- names(V(g.pubs))
non.guild.authors <- non.guild.authors[!(non.guild.authors %in%
																					 guild.authors)]
i <- rownames(full.au) %in% guild.authors
j <- rownames(full.au) %in% non.guild.authors

full.au$guild.member <- NA
full.au$guild.member[i] <- "yes"
full.au$guild.member[j] <- "no"
full.au$guild.member <- factor(full.au$guild.member)
```

Next, we compare the productivity of guilds to that of 'guilds' compiled from random individuals. We define the following function to do the calculations.

The first function calculates several measures of guild productivity. Guild productivity is measured for both the number of papers and citations. Guilds are characterised by (i) the total number of papers (and their citations) produced by the guild as a whole, (ii) the total number of papers (and their citations) produced by the guild members (in this case papers and citations can overlap between members), and (iii) the weighted numbers of papers (and their citations) produced by the members according to several scheme.

The second function generates `n.rep` guilds randomly compiled from authors not being member of any guilds and then calculates the above measures for each random guild.

The third function performs a randomisation test to see if the given measure for the focal guild differs from that of the random guilds.

The fourth function calculates the quantile of the guild measures relative to the distribution of random guild values.

```julia
guild.productivity <- function(guild, cikkek=papers.ls, szerzok=ee.prod) {
	g.cikkek <- cikkek[guild]
	p <- lapply(g.cikkek, function(sz) {x <- sz$citations; names(x) <- sz$id; x})
	pp <- lapply(g.cikkek, function(sz) {x <- sz$ranks; names(x) <- sz$id; x})
	d <- data.frame(n=unlist(sapply(p, names)), v=unlist(p), vv=unlist(pp))
	ud <- unique(d)
	r <- numeric()
	r["g.papers"] <- nrow(ud)
	r["g.citations"] <- sum(ud$v)
	r["g.D1"] <- sum(ud$vv == "D1")
	r["m.papers"] <- sum(szerzok[guild, "n.papers"])
	r["m.citations"] <- sum(szerzok[guild, "n.citations"])
	r["m.D1"] <- sum(szerzok[guild, "n.D1"])
	n <- names(szerzok)
	n <- n[grep("^w.*papers$", n)]
	for(nn in n) {
		r[nn] <- sum(szerzok[guild, nn])
		nc <- sub("papers$", "citations", nn)
		r[nc] <- sum(szerzok[guild, nc])
		nd <- sub("papers$", "D1", nn)
		r[nd] <- sum(szerzok[guild, nd])
	}
	r/length(guild)
}
rnd.guild.productivity <- function(guild.size, n.rep=1000, cikkek=papers.ls,
																		szerzok=ee.prod) {
	n.c.a <- rownames(szerzok)[szerzok$guild.member == "no"]
	res <- matrix(0, ncol=21, nrow=n.rep)
	for(i in 1:n.rep) {
		s <- sample(n.c.a, guild.size)
		r <- guild.productivity(s, cikkek=cikkek, szerzok=szerzok)
		res[i,] <- r
	}
	colnames(res) <- names(r)
	res
}
guild.productivity.test <- function(guild, p.value=0.05, cikkek=papers.ls,
																		szerzok=ee.prod) {
	res.g <- guild.productivity(guild, cikkek=cikkek, szerzok=szerzok)
	res.r <- rnd.guild.productivity(length(guild), 999, cikkek, szerzok)
	res.r <- rbind(res.g, res.r)
	value <- res.g
	l.crit <- numeric(ncol(res.r))
	u.crit <- numeric(ncol(res.r))
	for(i in 1:ncol(res.r)) {
		p <- quantile(res.r[,i], c(p.value/2, 1-p.value/2))
		l.crit[i] <- p[1]
		u.crit[i] <- p[2]
	}
	data.frame(value=res.g, l.crit=l.crit, u.crit=u.crit,
						 sig.mark=ifelse(l.crit < res.g & res.g < u.crit, "", "*"),
						 variable=names(res.g))
}
guild.productivity.q <- function(guild, p.value=0.05, n.rep=999,
																 cikkek=papers.ls, szerzok=ee.prod) {
	res.g <- guild.productivity(guild, cikkek=cikkek, szerzok=szerzok)
	res.r <- rnd.guild.productivity(length(guild), n.rep, cikkek, szerzok)
	res.r <- rbind(res.g, res.r)
	d <- numeric(ncol(res.r))
	m <- numeric(ncol(res.r))
	for(i in 1:ncol(res.r)) {
		d[i] <- sum(res.r[,i] > res.g[i])
		m[i] <- mean(res.r[,i])
	}
	data.frame(measure=names(res.g), value=res.g, rnd.mean=m,
						 quantile=d/nrow(res.r), guild.size=length(guild))
}
```

In the following we calculate the statistics for the guilds identified previously.

```julia
l.memb.red <- tapply(names(memb.red), memb.red, unique)
l <- sapply(l.memb.red, length)
i <- names(l.memb.red)[order(l, decreasing=TRUE)]
l.memb.red <- l.memb.red[i]
gd.file <- "guild_measures.Rdata"
if(file.exists(gd.file)) {
	load(gd.file)
} else {
	cl.p <- lapply(l.memb.red, guild.productivity.q, szerzok=full.au)
	cl.df <- data.frame()
	for(n in names(cl.p)) {
		l <- cl.p[[n]]
		l$guild <- n
		rownames(l) <- NULL
		cl.df <- rbind(cl.df, l)
	}
	cl.df$sig <- ifelse(cl.df$quantile > 0.975 | cl.df$quantile < 0.025, "*", "")
	cl.df$g2rnd <- cl.df$value/cl.df$rnd.mean
	save(cl.df, file=gd.file)
}
```

And finally, some analyses.

```julia
cits <- grepl("papers$", cl.df$measure)
m.papers <- lmer(log10(g2rnd) ~ measure + (1|guild), cl.df, subset=cits)
drop1(m.papers, test="Chisq")
summary(m.papers)
(em.p <- emmeans(m.papers, ~measure, type="response"))
pairs(em.p)
plot(em.p, horizontal=FALSE, xlab="measures", ylab="ratio of focal to random",
		 comparison=TRUE)
```

The result of the mixed effect model clearly shows that in guilds members achieve higher success if one considers success as the sum of the achievements documented individually (here number of papers) than when one measures the composite productivity of the whole guild. Even using the weighted individual measures does not help this situation. This analyses were based on all guilds identified.

```julia
cits <- grepl("citations$", cl.df$measure)
m.citations <- lmer(log10(g2rnd+1) ~ measure + (1|guild), cl.df, subset=cits)
drop1(m.citations, test="Chisq")
summary(m.citations)
(em.c <- emmeans(m.citations, ~measure, type="response"))
pairs(em.c)
plot(em.c, horizontal=FALSE, xlab="measures", ylab="ratio of focal to random",
		 comparison=TRUE)
```

Analysing the citation measures shows the same patterns, guild members have higher success if we consider their productivity independently from each other.

```julia
cits <- grepl("D1$", cl.df$measure)
m.D1 <- lmer(log10(g2rnd+1) ~ measure + (1|guild), cl.df, subset=cits)
drop1(m.D1, test="Chisq")
summary(m.D1)
(em.d <- emmeans(m.D1, ~measure, type="response"))
pairs(em.d)
plot(em.d, horizontal=FALSE, xlab="measures", ylab="ratio of focal to random",
		 comparison=TRUE)
```


We now analyse only those guilds which show higher than random productivity on the basis of the unweighted individual measure.

```julia
i <- grepl("^m\\.", cl.df$measure) & cl.df$g2rnd > 1
success.guilds <- as.character(cl.df$guild)[i]
i <- cl.df$guild %in% success.guilds
m.1 <- lmer(log10(g2rnd) ~ measure + (1|guild), cl.df, subset=i & !cits)
drop1(m.1, test="Chisq")
summary(m.1)
(em.1 <- emmeans(m.1, ~measure, type="response"))
pairs(em.1)
plot(em.1, horizontal=FALSE, xlab="measures", ylab="ratio of focal to random",
		 comparison=TRUE)
```

We arrived at the same conclusions; being part of a guild is advantageous if individual measures are considered.

### Diversity in cartels

A major difference between manufactures (groups where individuals with different skills work together) and cartels (group of individuals collude to increase publication counts) is the diversity of within group expertise. In manufactures one would expect a high diversity while in cartels individuals do the same. A further distinction can be the diversity of affiliations. In manufactures individuals can come from many different places, while in cartels they likely come from the same place. To make a distinction between these two work models we first download data about the authors and then analyse their expertise and their affiliation.

#### Download individuals' data

```julia
download.MTMT.authors.batch(as.character(author.info$mtmt.id))
```

Get the speciality.

```julia
json.list <- list.files(path="MTMT-downloads/", pattern=".*-author.json$",
												full.names=TRUE)
author.ls <- list()
for(f in json.list) {
	mtmt.id <- sub(".*\\/([0-9]+)_.*", "\\1", f)
	author.ls[[mtmt.id]] <- read.MTMT(f)
}
sci.fields <- sapply(author.ls,
										 function(a) if(is.null(a$auxName)) {NA} else {a$auxName})
sci.fields <- gsub("\\<a ", "", sci.fields)
sci.fields <- gsub("\\<Ã©s\\>", "", sci.fields)
sci.fields <- tolower(gsub("[-,)(]", "", sci.fields))
sci.fields <- gsub(" +", " ", sci.fields)
sci.ta <- sort(table(sci.fields))
names(sci.fields) <- sapply(as.numeric(names(sci.fields)), hash.id)
author.info$scientific.field <- NA
author.info[names(sci.fields), "scientific.field"] <- sci.fields
```

### Analyses of members

```julia
(w.papers <- wilcox.test(n.papers ~ guild.member, full.au))
(wi.ec.papers <- wilcox.test(w.ec.papers ~ guild.member, full.au))
layout(matrix(1:2, ncol=2))
boxplot(n.papers ~ guild.member, full.au, log="y", xlab="guild member",
				ylab="number of papers",
				main=paste("Wilcoxon p = ", round(w.papers$p.value, 3)))
boxplot(I(w.ec.papers+1) ~ guild.member, full.au, log="y",
				xlab="guild member", ylab="weighted number of papers",
				main=paste("Wilcoxon p = ", round(wi.ec.papers$p.value, 3)))
layout(1)

(w.citations <- wilcox.test(n.citations ~ guild.member, full.au))
(wi.ec.citations <- wilcox.test(w.ec.citations ~ guild.member, full.au))
layout(matrix(1:2, ncol=2))
boxplot(I(n.citations+1) ~ guild.member, full.au, log="y",
				xlab="guild member", ylab="number of citations",
				main=paste("Wilcoxon p = ", round(w.citations$p.value, 3)))
boxplot(I(w.ec.citations+1) ~ guild.member, full.au, log="y",
				xlab="guild member", ylab="weighted number of citations",
				main=paste("Wilcoxon p = ", round(wi.ec.citations$p.value, 3)))
layout(1)

(w.D1 <- wilcox.test(n.D1 ~ guild.member, full.au))
(wi.ec.D1 <- wilcox.test(w.ec.D1 ~ guild.member, full.au))
layout(matrix(1:2, ncol=2))
boxplot(I(n.D1+1) ~ guild.member, full.au, log="y",
				xlab="guild member", ylab="number of D1 papers",
				main=paste("Wilcoxon p = ", round(w.D1$p.value, 3)))
boxplot(I(w.ec.D1+1) ~ guild.member, full.au, log="y",
				xlab="guild member", ylab="weighted number of D1 papers",
				main=paste("Wilcoxon p = ", round(wi.ec.D1$p.value, 3)))
layout(1)
```

Authors in guild are more affected by weighting productivity.

Next we investigate, how the authors rank changes if switch from unweighted to weighted publication measures.

```julia
par(xpd=FALSE)
r.n.papers <- rank(full.au$n.papers)
r.flae.papers <- rank(full.au$w.flae.papers)
d.r <- r.flae.papers - r.n.papers
(w <- wilcox.test(d.r ~ guild.member, full.au))
boxplot(d.r ~ guild.member, full.au, xlab="guild member",
				ylab="change in rank: number of papers",
				main=paste("Wilcoxon p =", round(w$p.value, 3)))
abline(h=0, lty=2)

r.n.citations <- rank(full.au$n.citations)
r.flae.citations <- rank(full.au$w.flae.citations)
d.r <- r.flae.citations - r.n.citations
(w <- wilcox.test(d.r ~ guild.member, full.au))
boxplot(d.r ~ guild.member, full.au, xlab="guild member",
				ylab="change in rank: number of citations",
				main=paste("Wilcoxon p =", round(w$p.value, 4)))
abline(h=0, lty=2)

r.n.D1 <- rank(full.au$n.D1)
r.flae.D1 <- rank(full.au$w.flae.D1)
d.r <- r.flae.D1 - r.n.D1
(w <- wilcox.test(d.r ~ guild.member, full.au))
boxplot(d.r ~ guild.member, full.au, xlab="guild member",
				ylab="change in rank: number of D1 papers",
				main=paste("Wilcoxon p =", round(w$p.value, 4)))
abline(h=0, lty=2)
```

Here below, compare how the ranks of authors change when switching from unweighted to weighted measures, but now we compare authors of similar ranks.

```julia
o <- order(r.n.papers)
cm <- full.au$guild.member[o]
dr <- d.r[o]
i.c <- (1:nrow(full.au))[cm == "yes"]
i.c <- i.c[!is.na(i.c)]
i.p <- i.c - 1
i.p <- i.p[cm[i.p] == "no"]
i.n <- i.c + 1
i.n <- i.n[cm[i.n] == "no"]
(w <- wilcox.test(dr[i.c], c(dr[i.n],dr[i.p])))
boxplot(list(no=c(dr[i.p], dr[i.n]), yes=dr[i.c]), xlab="guild member",
				ylab="change in rank: number of papers",
				main=paste("Wilcoxon p =", round(w$p.value, 4)))
abline(h=0, lty=2)

o <- order(r.n.citations)
cm <- full.au$guild.member[o]
dr <- d.r[o]
i.c <- (1:nrow(full.au))[cm == "yes"]
i.c <- i.c[!is.na(i.c)]
i.p <- i.c - 1
i.p <- i.p[cm[i.p] == "no"]
i.n <- i.c + 1
i.n <- i.n[cm[i.n] == "no"]
(w <- wilcox.test(dr[i.c], c(dr[i.n],dr[i.p])))
boxplot(list(no=c(dr[i.p], dr[i.n]), yes=dr[i.c]), xlab="guild member",
				ylab="change in rank: number of citations",
				main=paste("Wilcoxon p =", round(w$p.value, 4)))
abline(h=0, lty=2)

o <- order(r.n.D1)
cm <- full.au$guild.member[o]
dr <- d.r[o]
i.c <- (1:nrow(full.au))[cm == "yes"]
i.c <- i.c[!is.na(i.c)]
i.p <- i.c - 1
i.p <- i.p[cm[i.p] == "no"]
i.n <- i.c + 1
i.n <- i.n[cm[i.n] == "no"]
(w <- wilcox.test(dr[i.c], c(dr[i.n],dr[i.p])))
boxplot(list(no=c(dr[i.p], dr[i.n]), yes=dr[i.c]), xlab="guild member",
				ylab="change in rank: number of D1 papers",
				main=paste("Wilcoxon p =", round(w$p.value, 4)))
abline(h=0, lty=2)

```

#### Correlates of being a strong component member

```julia
library(randomForest)
ee.prod <- full.au[!is.na(full.au$guild.member),]
ee.RF <- randomForest(guild.member ~ ., ee.prod, importance=TRUE)
varImpPlot(ee.RF)
l.ee.prod <- as.data.frame(lapply(ee.prod[,-ncol(ee.prod)],
																	function(x) log10(x+1)))
l.ee.prod$betweenness <- betweenness(g.pubs)
l.ee.prod$strength <- strength(g.pubs)
l.ee.prod$closeness <- closeness(g.pubs)
l.ee.prod$degree <- degree(g.pubs)
l.ee.prod$guild.member <- ee.prod$guild.member
l.ee.RF <- randomForest(guild.member ~ ., l.ee.prod, importance=TRUE)
varImpPlot(l.ee.RF)
p.cl <- predict(l.ee.RF)
table(p.cl, l.ee.prod$guild.member)
partialPlot(l.ee.RF, l.ee.prod, "strength")
```


# The dblp dataset

We downloaded the [dblp publication database](https://dblp.org), which covers computational sciences. Then we extracted all article records from the database `xml` which have a `doi.org` id. During the extraction we dropped all but the `author`, `year` and `doi` fields. The process is documented in `dblp-data/Makefile`. We use the resulted `dblp-data/dblp-articles.txt` here.

## Load the dblp dataset

Now we load the dataset.

```julia
dblp_file = joinpath(pwd(), "dblp-data", "dblp-articles.txt")
lines = read_dblprecords(dblp_file)
@time parecsdblp = recordpapers(lines, 2001, 2002);
@time dblppubmat, dblpauids = recs2pubmat(parecsdblp);
write_spmatrix("dblppubmat.csv", dblppubmat)
@time dblpcolmat = collaborationmatrix(dblppubmat);
write_spmatrix("dblpcolmat.csv", dblpcolmat)
@time dblpcolnet = collaborationgraph(dblpcolmat);
```

## Some description about the dataset

The dblp dataset contains more than one and half million records. The distribution of articles over the years is as follows.

```julia
years <- sapply(dblp.ls, function(y) y$year)
plot(table(years), ylab="number of articles")
```

The distribution of number of authors by articles.

```julia
n.authors <- sapply(dblp.ls, function(y) length(y$authors))
plot(table(n.authors))
plot.loglog(n.authors, pch=16, bty="l", xlab="number of authors per paper",
						ylab="frequency")
```

The number of distinct authors in the dataset:

```julia
d.authors <- unlist(lapply(dblp.ls, function(y) y$authors))
length(d.authors)
length(unique(d.authors))
t.authors <- table(d.authors)
t.authors <- sort(t.authors)
plot.loglog(as.numeric(t.authors), pch=16, bty="l",
		 xlab="number of papers per author", ylab="frequency")
```

## Reducing the dblp dataset

We have a huge number of articles so for the first time we concentrate only articles between 2001 and 2010.

```julia
years <- as.numeric(years)
i <- years >= 2001 & years <= 2010
sum(i)
dblp.red <- dblp.ls[i]
rm("dblp.ls")
gc()
```

This is still a huge number, but we make a try.

```julia
n.authors <- sapply(dblp.red, function(y) length(y$authors))
plot(table(n.authors))
plot.loglog(n.authors, pch=16, bty="l", xlab="number of authors per paper",
						ylab="frequency")
```


```julia
a.authors <- unlist(lapply(dblp.red, function(y) y$authors))
length(a.authors)
d.authors <- unique(a.authors)
length(d.authors)
t.authors <- table(a.authors)
t.authors <- sort(t.authors)
plot.loglog(as.numeric(t.authors), pch=16, bty="l",
		 xlab="number of papers per author", ylab="frequency")
```

## Process the article data

We are create a sparse matrix and fill it with the coauthorship data.

```julia
p.ls <- dblp.red[1:100]
d.authors <- unique(unlist(lapply(p.ls, function(y) y$authors)))
d.mat <- Matrix(0.0, ncol=length(d.authors), nrow=length(d.authors))
colnames(d.mat) <- d.authors
rownames(d.mat) <- d.authors
for(p in p.ls) {
	n.a <- length(p$authors)
	for(a in p$authors) {
		d.mat[a, a] <- d.mat[a, a] + 1
	}
	if(n.a > 1) {
		for(i in 1:(n.a-1)) {
			a1 <- p$authors[i]
			for(j in (i+1):n.a) {
				a2 <- p$authors[j]
				d.mat[a1, a2] <- d.mat[a1, a2] + 1
			}
		}
	}
}
for(i  in 1:(ncol(d.mat)-1)) {
	for(j in (i+1):ncol(d.mat)) {
		d.sec <- d.mat[i,j] + d.mat[j,i]
		if(d.sec > 0) {
			d.uni <- d.mat[i, i] + d.mat[j, j] - d.sec
			d.mat[i, j] <- d.sec/d.uni
			d.mat[j, i] <- 0
		}
	}
}
g.dblp <- graph.adjacency(d.mat, mode="upper", weighted=TRUE, diag=FALSE)
```



# End matters

```julia
sessionInfo()
```

<!-- vim: set foldmethod=syntax: -->
